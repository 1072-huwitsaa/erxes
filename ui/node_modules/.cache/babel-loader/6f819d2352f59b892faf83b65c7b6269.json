{"ast":null,"code":"var DEBUG = false; // `true` to print debugging info.\n\nvar TIMER = false; // `true` to time calls to `parse()` and print the results.\n\nvar debug = require('./debug')('parse');\n\nvar lex = require('./lexer');\n\nexports = module.exports = parse;\n\nvar _comments; // Whether comments are allowed.\n\n\nvar _depth; // Current block nesting depth.\n\n\nvar _position; // Whether to include line/column position.\n\n\nvar _tokens; // Array of lexical tokens.\n\n/**\r\n * Convert a CSS string or array of lexical tokens into a `stringify`-able AST.\r\n *\r\n * @param {String} css CSS string or array of lexical token\r\n * @param {Object} [options]\r\n * @param {Boolean} [options.comments=false] allow comment nodes in the AST\r\n * @returns {Object} `stringify`-able AST\r\n */\n\n\nfunction parse(css, options) {\n  var start; // Debug timer start.\n\n  options || (options = {});\n  _comments = !!options.comments;\n  _position = !!options.position;\n  _depth = 0; // Operate on a copy of the given tokens, or the lex()'d CSS string.\n\n  _tokens = Array.isArray(css) ? css.slice() : lex(css);\n  var rule;\n  var rules = [];\n  var token;\n  TIMER && (start = Date.now());\n\n  while (token = next()) {\n    rule = parseToken(token);\n    rule && rules.push(rule);\n  }\n\n  TIMER && debug('ran in', Date.now() - start + 'ms');\n  return {\n    type: \"stylesheet\",\n    stylesheet: {\n      rules: rules\n    }\n  };\n} // -- Functions --------------------------------------------------------------\n\n/**\r\n * Build an AST node from a lexical token.\r\n *\r\n * @param {Object} token lexical token\r\n * @param {Object} [override] object hash of properties that override those\r\n *   already in the token, or that will be added to the token.\r\n * @returns {Object} AST node\r\n */\n\n\nfunction astNode(token, override) {\n  override || (override = {});\n  var key;\n  var keys = ['type', 'name', 'value'];\n  var node = {}; // Avoiding [].forEach for performance reasons.\n\n  for (var i = 0; i < keys.length; ++i) {\n    key = keys[i];\n\n    if (token[key]) {\n      node[key] = override[key] || token[key];\n    }\n  }\n\n  keys = Object.keys(override);\n\n  for (i = 0; i < keys.length; ++i) {\n    key = keys[i];\n\n    if (!node[key]) {\n      node[key] = override[key];\n    }\n  }\n\n  if (_position) {\n    node.position = {\n      start: token.start,\n      end: token.end\n    };\n  }\n\n  DEBUG && debug('astNode:', JSON.stringify(node, null, 2));\n  return node;\n}\n/**\r\n * Remove a lexical token from the stack and return the removed token.\r\n *\r\n * @returns {Object} lexical token\r\n */\n\n\nfunction next() {\n  var token = _tokens.shift();\n\n  DEBUG && debug('next:', JSON.stringify(token, null, 2));\n  return token;\n} // -- Parse* Functions ---------------------------------------------------------\n\n/**\r\n * Convert an @-group lexical token to an AST node.\r\n *\r\n * @param {Object} token @-group lexical token\r\n * @returns {Object} @-group AST node\r\n */\n\n\nfunction parseAtGroup(token) {\n  _depth = _depth + 1; // As the @-group token is assembled, relevant token values are captured here\n  // temporarily. They will later be used as `tokenize()` overrides.\n\n  var overrides = {};\n\n  switch (token.type) {\n    case 'font-face':\n    case 'viewport':\n      overrides.declarations = parseDeclarations();\n      break;\n\n    case 'page':\n      overrides.prefix = token.prefix;\n      overrides.declarations = parseDeclarations();\n      break;\n\n    default:\n      overrides.prefix = token.prefix;\n      overrides.rules = parseRules();\n  }\n\n  return astNode(token, overrides);\n}\n/**\r\n * Convert an @import lexical token to an AST node.\r\n *\r\n * @param {Object} token @import lexical token\r\n * @returns {Object} @import AST node\r\n */\n\n\nfunction parseAtImport(token) {\n  return astNode(token);\n}\n/**\r\n * Convert an @charset token to an AST node.\r\n *\r\n * @param {Object} token @charset lexical token\r\n * @returns {Object} @charset node\r\n */\n\n\nfunction parseCharset(token) {\n  return astNode(token);\n}\n/**\r\n * Convert a comment token to an AST Node.\r\n *\r\n * @param {Object} token comment lexical token\r\n * @returns {Object} comment node\r\n */\n\n\nfunction parseComment(token) {\n  return astNode(token, {\n    text: token.text\n  });\n}\n\nfunction parseNamespace(token) {\n  return astNode(token);\n}\n/**\r\n * Convert a property lexical token to a property AST node.\r\n *\r\n * @returns {Object} property node\r\n */\n\n\nfunction parseProperty(token) {\n  return astNode(token);\n}\n/**\r\n * Convert a selector lexical token to a selector AST node.\r\n *\r\n * @param {Object} token selector lexical token\r\n * @returns {Object} selector node\r\n */\n\n\nfunction parseSelector(token) {\n  function trim(str) {\n    return str.trim();\n  }\n\n  return astNode(token, {\n    type: 'rule',\n    selectors: token.text.split(',').map(trim),\n    declarations: parseDeclarations(token)\n  });\n}\n/**\r\n * Convert a lexical token to an AST node.\r\n *\r\n * @returns {Object|undefined} AST node\r\n */\n\n\nfunction parseToken(token) {\n  switch (token.type) {\n    // Cases are listed in roughly descending order of probability.\n    case 'property':\n      return parseProperty(token);\n\n    case 'selector':\n      return parseSelector(token);\n\n    case 'at-group-end':\n      _depth = _depth - 1;\n      return;\n\n    case 'media':\n    case 'keyframes':\n      return parseAtGroup(token);\n\n    case 'comment':\n      if (_comments) {\n        return parseComment(token);\n      }\n\n      break;\n\n    case 'charset':\n      return parseCharset(token);\n\n    case 'import':\n      return parseAtImport(token);\n\n    case 'namespace':\n      return parseNamespace(token);\n\n    case 'font-face':\n    case 'supports':\n    case 'viewport':\n    case 'document':\n    case 'page':\n      return parseAtGroup(token);\n  }\n\n  DEBUG && debug('parseToken: unexpected token:', JSON.stringify(token));\n} // -- Parse Helper Functions ---------------------------------------------------\n\n/**\r\n * Iteratively parses lexical tokens from the stack into AST nodes until a\r\n * conditional function returns `false`, at which point iteration terminates\r\n * and any AST nodes collected are returned.\r\n *\r\n * @param {Function} conditionFn\r\n *   @param {Object} token the lexical token being parsed\r\n *   @returns {Boolean} `true` if the token should be parsed, `false` otherwise\r\n * @return {Array} AST nodes\r\n */\n\n\nfunction parseTokensWhile(conditionFn) {\n  var node;\n  var nodes = [];\n  var token;\n\n  while ((token = next()) && conditionFn && conditionFn(token)) {\n    node = parseToken(token);\n    node && nodes.push(node);\n  } // Place an unused non-`end` lexical token back onto the stack.\n\n\n  if (token && token.type !== 'end') {\n    _tokens.unshift(token);\n  }\n\n  return nodes;\n}\n/**\r\n * Convert a series of tokens into a sequence of declaration AST nodes.\r\n *\r\n * @returns {Array} declaration nodes\r\n */\n\n\nfunction parseDeclarations() {\n  return parseTokensWhile(function (token) {\n    return token.type === 'property' || token.type === 'comment';\n  });\n}\n/**\r\n * Convert a series of tokens into a sequence of rule nodes.\r\n *\r\n * @returns {Array} rule nodes\r\n */\n\n\nfunction parseRules() {\n  return parseTokensWhile(function () {\n    return _depth;\n  });\n}","map":null,"metadata":{},"sourceType":"script"}